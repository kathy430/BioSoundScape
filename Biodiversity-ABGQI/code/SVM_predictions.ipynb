{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM+g2HXtOh3+nLYjUvYSjf8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **SVM Predictions**\n","\n","This notebook uses Colin's trained CNN model as a feature extractor to train a Anthropophony, Biophony, Geophony, and Interference SVM. The labels for the 1 min WAV file are obtained from a CSV file."],"metadata":{"id":"gJsoS_nWRYs6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1EzamBGjOuri"},"outputs":[],"source":["# Importing Drive for Google Colab\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import tensorflow as tf\n","'''\n","%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n","%env CUDA_VISIBLE_DEVICES=1\n","'''\n","from tensorflow.keras.models import Sequential\n","import IPython.display as display\n","from PIL import Image\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import pickle\n","\n","import sklearn.svm\n","from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","\n","import os\n","import pathlib\n","import glob\n","import joblib\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n","AUTOTUNE = tf.data.experimental.AUTOTUNE\n","tf.__version__"],"metadata":{"id":"kRRSniSwO2-y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# User Inputs"],"metadata":{"id":"AAKSNyFXO69B"}},{"cell_type":"code","source":["# trained models\n","wd = '/content/drive/Shareddrives/cs479 ABGQI/CQuinn8-ABGQI-CNN-93420d1/1_cnn_training-py/results/'\n","checkpoint_path = os.path.join(wd, 'ABGQI-CNN') # IMGNET + S2L CVFOLDS\n","\n","# data dir for images to predict class\n","data_path = '/content/drive/Shareddrives/cs479 ABGQI/CQuinn8-ABGQI-CNN-93420d1/5_ABGQI-CNN_deployment/data/1min_data/'\n","\n","# number of cores\n","cpus = 2\n","\n","print(checkpoint_path)"],"metadata":{"id":"vQD_FgSOO-xU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dimensions of our images\n","img_width, img_height, img_depth = 224, 224, 3 # 224 pixels x 224 pixels x 3 bands (RGB)"],"metadata":{"id":"PPjMd06vPPmf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# number of images to predict at once\n","batch_size = 50"],"metadata":{"id":"4hrQT2_aPQrx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Functions"],"metadata":{"id":"S-d8ona5PVPu"}},{"cell_type":"code","source":["# function to read in a file path\n","def process_path(file_path, IMG_HEIGHT, IMG_WIDTH):\n","    # load the raw data from the file as a string\n","    img = tf.io.read_file(file_path)\n","    img = decode_img(img, IMG_HEIGHT, IMG_WIDTH)\n","    return img #, label\n","\n","# function that interpretes image after being provided a path in process_path\n","def decode_img(img, IMG_HEIGHT, IMG_WIDTH):\n","  # convert the compressed string to a 3D uint8 tensor\n","  img = tf.image.decode_jpeg(img, channels=3)\n","  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n","  img = tf.image.convert_image_dtype(img, tf.float32)\n","  # resize the image to the desired size.\n","  return tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])"],"metadata":{"id":"lvvusStEPROz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load trained model checkpoint\n"],"metadata":{"id":"my_c140PPbiZ"}},{"cell_type":"code","source":["# LOAD MODEL\n","model = tf.keras.models.load_model(checkpoint_path)\n","model.summary()"],"metadata":{"id":"6mngBAAfPaKW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# remove last layer in old model\n","modified_model = model # save in new model\n","modified_model.pop()\n","modified_model.summary()"],"metadata":{"id":"3e9unmiTPfXO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pick last layer as a feature extractor\n","for i in range(len(modified_model.layers)):\n","    layer = model.layers[i]\n","    print(i, layer.name, layer.output.shape)\n","\n","feature_extractor = tf.keras.Model(inputs=modified_model.inputs, outputs=modified_model.layers[1].output)"],"metadata":{"id":"V1Mq3QaKPkYk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Set up sigmoid prediction"],"metadata":{"id":"UQP7ERt6PoFI"}},{"cell_type":"code","source":["png_dirs = []\n","for filename in os.listdir(data_path):\n","    if os.path.isdir(os.path.join(data_path, filename)):\n","        png_dirs.append(filename)\n","if len(png_dirs) == 0:\n","    png_dirs.append(data_path)"],"metadata":{"id":"Fcu5u1RfPp1C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["png_dirs"],"metadata":{"id":"1Bp--DWhPthB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(checkpoint_path)"],"metadata":{"id":"NmgAuq7pPwiS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Feature extraction"],"metadata":{"id":"rUvpsr-3Qjd1"}},{"cell_type":"code","source":["# iterate through each directory (class or wav dir with mfccs within)\n","cnn_features = []\n","for i,f in enumerate(png_dirs):\n","    temp_dir = f\n","    pngs = glob.glob(temp_dir + \"/*.png\")\n","    #temp_class = temp_dir.split('/')[-2] # png parent dir\n","    temp_class = temp_dir\n","    print()\n","    print(\"Number of pngs\", len(pngs))\n","\n","    # list to hold each dir of predictions (either all pngs in class or mfccs in wav)\n","    sigmoid_pred_lst = []\n","\n","    # iterate through each melspec in file directory\n","    for j in range(len(pngs)):\n","        print(j)\n","        temp_png = pngs[j] # png\n","\n","        #check if pkl (prediction) for file exists\n","        if os.path.isfile(out_path_temp):\n","            pass # skip file\n","\n","        # enter prediction\n","        else:\n","            # process image\n","            img_ = process_path(temp_png, IMG_HEIGHT = img_height, IMG_WIDTH = img_width)\n","            img_ = tf.reshape(img_, shape= (1, img_height, img_width, img_depth))\n","\n","            # get predictions (features)\n","            features = modified_model.predict(img_, verbose=0, steps=1, callbacks=None, max_queue_size=10,\n","                  workers=cpus, use_multiprocessing=False)\n","            features = features.squeeze() #remove excess dimension\n","\n","            cnn_features.append(features)"],"metadata":{"id":"gcSKGbSwPzh5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# SVM"],"metadata":{"id":"pxkoCQ9gQmfy"}},{"cell_type":"code","source":["# set up labels for svm\n","anthro_df = pd.read_csv('/content/drive/Shareddrives/cs479 ABGQI/CQuinn8-ABGQI-CNN-93420d1/arbimon_soundscape_composition_date_sorted.csv',\n","                    header = 0,\n","                    usecols = ['Anthropophony'])\n","\n","bioph_df = pd.read_csv('/content/drive/Shareddrives/cs479 ABGQI/CQuinn8-ABGQI-CNN-93420d1/arbimon_soundscape_composition_date_sorted.csv',\n","                    header = 0,\n","                    usecols = ['Biophony'])\n","\n","geoph_df = pd.read_csv('/content/drive/Shareddrives/cs479 ABGQI/CQuinn8-ABGQI-CNN-93420d1/arbimon_soundscape_composition_date_sorted.csv',\n","                    header = 0,\n","                    usecols = ['Geophony'])\n","\n","interf_df = pd.read_csv('/content/drive/Shareddrives/cs479 ABGQI/CQuinn8-ABGQI-CNN-93420d1/arbimon_soundscape_composition_date_sorted.csv',\n","                    header = 0,\n","                    usecols = ['Interference'])\n","\n","anthro_labels = np.array(anthro_df).ravel()\n","bioph_labels = np.array(bioph_df).ravel()\n","geoph_labels = np.array(geoph_df).ravel()\n","interf_labels = np.array(interf_df).ravel()\n","\n","print(anthro_labels.shape, bioph_labels.shape, geoph_labels.shape, interf_labels.shape)"],"metadata":{"id":"TGRVASqDQiTP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# split data into train and test for 4 different svms\n","features = np.array(cnn_features)\n","print(features.shape)\n","\n","anthro_train, anthro_test, anthro_train_labels, anthro_test_labels = train_test_split(features, anthro_labels, test_size=0.4)\n","bioph_train, bioph_test, bioph_train_labels, bioph_test_labels = train_test_split(features, bioph_labels, test_size=0.4)\n","geoph_train, geoph_test, geoph_train_labels, geoph_test_labels = train_test_split(features, geoph_labels, test_size=0.4)\n","interf_train, interf_test, interf_train_labels, interf_test_labels = train_test_split(features, interf_labels, test_size=0.4)"],"metadata":{"id":"6KoL26GMQruk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# svm model\n","svm_model = sklearn.svm.SVC(kernel='linear', C=1e6, probability=True)"],"metadata":{"id":"kmY8zfPJQ5es"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# run anthropophony svm\n","svm_model.fit(anthro_train, anthro_train_labels)\n","anthro_accuracy = svm_model.score(anthro_test, anthro_test_labels)\n","\n","confus_mat = metrics.confusion_matrix(anthro_test_labels, svm_model.predict(anthro_test))\n","cm_dis = metrics.ConfusionMatrixDisplay(confusion_matrix=confus_mat, display_labels=[False, True])\n","cm_dis.plot(cmap=\"YlGnBu\")\n","plt.title(\"Anthropophony Confusion Matrix\")\n","plt.ylabel(\"Actual\")\n","plt.xlabel(\"Predicted\")\n","plt.show()\n","plt.clf() # Clear the plot for the next matrix"],"metadata":{"id":"NxcZViohQtTE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# run biophony svm\n","svm_model.fit(bioph_train, bioph_train_labels)\n","bioph_accuracy = svm_model.score(bioph_test, bioph_test_labels)\n","\n","confus_mat = metrics.confusion_matrix(bioph_test_labels, svm_model.predict(bioph_test))\n","cm_dis = metrics.ConfusionMatrixDisplay(confusion_matrix=confus_mat, display_labels=[False, True])\n","cm_dis.plot(cmap=\"YlGnBu\")\n","plt.title(\"Biophony Confusion Matrix\")\n","plt.ylabel(\"Actual\")\n","plt.xlabel(\"Predicted\")\n","plt.show()\n","plt.clf() # Clear the plot for the next matrix"],"metadata":{"id":"PReVKxthQu9u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# run geophony svm\n","svm_model.fit(geoph_train, geoph_train_labels)\n","geoph_accuracy = svm_model.score(geoph_test, geoph_test_labels)\n","\n","confus_mat = metrics.confusion_matrix(geoph_test_labels, svm_model.predict(geoph_test))\n","cm_dis = metrics.ConfusionMatrixDisplay(confusion_matrix=confus_mat, display_labels=[False, True])\n","cm_dis.plot(cmap=\"YlGnBu\")\n","plt.title(\"Geophony Confusion Matrix\")\n","plt.ylabel(\"Actual\")\n","plt.xlabel(\"Predicted\")\n","plt.show()\n","plt.clf() # Clear the plot for the next matrix"],"metadata":{"id":"sEY553HGRFKf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# run interference svm\n","svm_model.fit(interf_train, interf_train_labels)\n","interf_accuracy = svm_model.score(interf_test, interf_test_labels)\n","\n","confus_mat = metrics.confusion_matrix(interf_test_labels, svm_model.predict(interf_test))\n","cm_dis = metrics.ConfusionMatrixDisplay(confusion_matrix=confus_mat, display_labels=[False, True])\n","cm_dis.plot(cmap=\"YlGnBu\")\n","plt.title(\"Interference Confusion Matrix\")\n","plt.ylabel(\"Actual\")\n","plt.xlabel(\"Predicted\")\n","plt.show()\n","plt.clf() # Clear the plot for the next matrix"],"metadata":{"id":"seXSytctRKVa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Anthropophony accuracy: \", anthro_accuracy)\n","print(\"Biophony accuracy: \", bioph_accuracy)\n","print(\"Geophony accuracy: \", geoph_accuracy)\n","print(\"Interference accuracy: \", interf_accuracy)"],"metadata":{"id":"3E56BX2tYHaO"},"execution_count":null,"outputs":[]}]}